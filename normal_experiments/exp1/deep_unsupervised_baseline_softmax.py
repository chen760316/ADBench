"""
æ— ç›‘ç£ç¦»ç¾¤å€¼æ£€æµ‹ç®—æ³•ä¿®å¤æ•ˆæœæµ‹è¯•
"""
import re

import pandas as pd
import  os
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
import numpy as np
import torch
from sklearn import svm
from sklearn.neighbors import KNeighborsClassifier
from sklearn.impute import KNNImputer
from lime.lime_tabular import LimeTabularExplainer
from tensorflow.python.ops.metrics_impl import accuracy

from adbench.baseline.DAGMM.run import DAGMM


pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', None)
pd.set_option('display.width', None)
np.set_printoptions(threshold=np.inf)



def run(path):
    data = pd.read_csv(path)
    # å¦‚æœæ•°æ®é‡è¶…è¿‡20000è¡Œï¼Œå°±éšæœºé‡‡æ ·åˆ°20000è¡Œ
    if len(data) > 20000:
        data= data.sample(n=20000, random_state=42)

    enc = LabelEncoder()
    label_name = data.columns[-1]

    # åŸå§‹æ•°æ®é›†Då¯¹åº”çš„Dataframe
    data[label_name] = enc.fit_transform(data[label_name])

    # æ£€æµ‹éæ•°å€¼åˆ—
    non_numeric_columns = data.select_dtypes(exclude=[np.number]).columns

    # ä¸ºæ¯ä¸ªéæ•°å€¼åˆ—åˆ›å»ºä¸€ä¸ª LabelEncoder å®ä¾‹
    encoders = {}
    for column in non_numeric_columns:
        encoder = LabelEncoder()
        data[column] = encoder.fit_transform(data[column])
        encoders[column] = encoder  # ä¿å­˜æ¯ä¸ªåˆ—çš„ç¼–ç å™¨ï¼Œä»¥ä¾¿å°†æ¥å¯èƒ½éœ€è¦è§£ç 
    X = data.values[:, :-1]
    y = data.values[:, -1]

    # æ‰¾åˆ°åˆ†ç±»ç‰¹å¾çš„åˆ—å
    categorical_columns = data.select_dtypes(exclude=['float']).columns[:-1]
    # è·å–åˆ†ç±»ç‰¹å¾å¯¹åº”çš„ç´¢å¼•
    categorical_features = [data.columns.get_loc(col) for col in categorical_columns]

    # ç»Ÿè®¡ä¸åŒå€¼åŠå…¶æ•°é‡
    unique_values, counts = np.unique(y, return_counts=True)

    # è¾“å‡ºç»“æœ
    # for value, count in zip(unique_values, counts):
    #     print(f"æ ‡ç­¾: {value}, æ•°é‡: {count}")

    # æ‰¾åˆ°æœ€å°æ ‡ç­¾çš„æ•°é‡
    min_count = counts.min()
    total_count = counts.sum()

    # è®¡ç®—æ¯”ä¾‹
    proportion = min_count / total_count
    #print(f"è¾ƒå°‘æ ‡ç­¾å æ®çš„æ¯”ä¾‹: {proportion:.4f}")
    min_count_index = np.argmin(counts)  # æ‰¾åˆ°æœ€å°æ•°é‡çš„ç´¢å¼•
    min_label = unique_values[min_count_index]  # å¯¹åº”çš„æ ‡ç­¾å€¼

    # section æ•°æ®ç‰¹å¾ç¼©æ”¾

    # å¯¹ä¸åŒç»´åº¦è¿›è¡Œæ ‡å‡†åŒ–
    X = StandardScaler().fit_transform(X)
    # è®°å½•åŸå§‹ç´¢å¼•
    original_indices = np.arange(len(X))
    X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(X, y, original_indices, test_size=0.3, random_state=1)
    # åŠ å…¥éšæœºå™ªå£°çš„æ¯”ä¾‹
    noise_level = 0.2
    # è®¡ç®—å™ªå£°æ•°é‡
    n_samples = X.shape[0]
    n_noise = int(noise_level * n_samples)
    # éšæœºé€‰æ‹©è¦æ·»åŠ å™ªå£°çš„æ ·æœ¬
    noise_indices = np.random.choice(n_samples, n_noise, replace=False)
    # æ·»åŠ é«˜æ–¯å™ªå£°åˆ°ç‰¹å¾
    X_copy = np.copy(X)
    X_copy[noise_indices] += np.random.normal(0, 1, (n_noise, X.shape[1]))
    # ä»å«å™ªæ•°æ®ä¸­ç”Ÿæˆè®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®
    X_train_copy = X_copy[train_indices]
    X_test_copy = X_copy[test_indices]
    feature_names = data.columns.values.tolist()
    combined_array = np.hstack((X_copy, y.reshape(-1, 1)))  # å°† y é‡æ–°è°ƒæ•´ä¸ºåˆ—å‘é‡å¹¶åˆå¹¶
    # æ·»åŠ å™ªå£°åçš„æ•°æ®é›†D'å¯¹åº”çš„Dataframe
    data_copy = pd.DataFrame(combined_array, columns=feature_names)
    # è®­ç»ƒé›†ä¸­æ·»åŠ äº†é«˜æ–¯å™ªå£°çš„æ ·æœ¬åœ¨åŸå§‹æ•°æ®é›†Dä¸­çš„ç´¢å¼•
    train_noise = np.intersect1d(train_indices, noise_indices)
    # æµ‹è¯•é›†ä¸­æ·»åŠ äº†é«˜æ–¯å™ªå£°çš„æ ·æœ¬åœ¨åŸå§‹æ•°æ®é›†Dä¸­çš„ç´¢å¼•
    test_noise = np.intersect1d(test_indices, noise_indices)
    # print("è®­ç»ƒé›†ä¸­çš„å™ªå£°æ ·æœ¬ä¸ºï¼š", train_noise)
    # print("æµ‹è¯•é›†ä¸­çš„å™ªå£°æ ·æœ¬ä¸ºï¼š", test_noise)

    # SECTION Mğ‘œ (ğ‘¡, D) é’ˆå¯¹å…ƒç»„å¼‚å¸¸çš„æ— ç›‘ç£å¼‚å¸¸æ£€æµ‹å™¨GOAD
    epochs = 1
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    n_trans = 64
    random_state = 42

    # choice DAGMMå¼‚å¸¸æ£€æµ‹å™¨
    out_clf = DAGMM(seed=random_state)
    out_clf.fit(X_train,y_train=np.repeat(0, X_train.shape[0]))
    out_clf_noise = DAGMM(seed=random_state)
    out_clf_noise.fit(X_train_copy,y_train=np.repeat(0, X_train_copy.shape[0]))


    # SECTION å€ŸåŠ©å¼‚å¸¸æ£€æµ‹å™¨ï¼Œåœ¨è®­ç»ƒé›†ä¸Šè¿›è¡Œå¼‚å¸¸å€¼æ£€æµ‹ã€‚
    #  ç»è¿‡æ£€éªŒï¼ŒåŠ å…¥é«˜æ–¯å™ªå£°ä¼šå½±å“å¼‚å¸¸å€¼åˆ¤åˆ«

    # subsection ä»åŸå§‹è®­ç»ƒé›†ä¸­æ£€æµ‹å‡ºå¼‚å¸¸å€¼ç´¢å¼•

    #print("*"*100)
    #train_scores = out_clf.decision_function(X_train)
    train_pred_labels = out_clf.predict_label(X_train,X_train)
    #print("è®­ç»ƒé›†ä¸­å¼‚å¸¸å€¼åˆ¤å®šé˜ˆå€¼ä¸ºï¼š", out_clf.threshold_)
    train_outliers_index = []
    #print("è®­ç»ƒé›†æ ·æœ¬æ•°ï¼š", len(X_train))
    for i in range(len(X_train)):
        if train_pred_labels[i] == 1:
            train_outliers_index.append(i)
    # è®­ç»ƒæ ·æœ¬ä¸­çš„å¼‚å¸¸å€¼ç´¢å¼•
    # print("è®­ç»ƒé›†ä¸­å¼‚å¸¸å€¼ç´¢å¼•ï¼š", train_outliers_index)
    # print("è®­ç»ƒé›†ä¸­çš„å¼‚å¸¸å€¼æ•°é‡ï¼š", len(train_outliers_index))
    # print("è®­ç»ƒé›†ä¸­çš„å¼‚å¸¸å€¼æ¯”ä¾‹ï¼š", len(train_outliers_index)/len(X_train))


    # subsection ä»åŸå§‹æµ‹è¯•é›†ä¸­æ£€æµ‹å‡ºå¼‚å¸¸å€¼ç´¢å¼•
    #test_scores = out_clf.decision_function(X_test)
    test_pred_labels = out_clf.predict_label(X_train, X_test)
    #print("æµ‹è¯•é›†ä¸­å¼‚å¸¸å€¼åˆ¤å®šé˜ˆå€¼ä¸ºï¼š", out_clf.threshold_)
    test_outliers_index = []
    #print("æµ‹è¯•é›†æ ·æœ¬æ•°ï¼š", len(X_test))
    for i in range(len(X_test)):
        if test_pred_labels[i] == 1:
            test_outliers_index.append(i)
    # è®­ç»ƒæ ·æœ¬ä¸­çš„å¼‚å¸¸å€¼ç´¢å¼•
    # print("æµ‹è¯•é›†ä¸­å¼‚å¸¸å€¼ç´¢å¼•ï¼š", test_outliers_index)
    # print("æµ‹è¯•é›†ä¸­çš„å¼‚å¸¸å€¼æ•°é‡ï¼š", len(test_outliers_index))
    # print("æµ‹è¯•é›†ä¸­çš„å¼‚å¸¸å€¼æ¯”ä¾‹ï¼š", len(test_outliers_index)/len(X_test))

    # section ä»åŠ å™ªæ•°æ®é›†çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­æ£€æµ‹å‡ºçš„å¼‚å¸¸å€¼

    # subsection ä»åŠ å™ªè®­ç»ƒé›†ä¸­æ£€æµ‹å‡ºå¼‚å¸¸å€¼ç´¢å¼•
    #train_scores_noise = out_clf_noise.decision_function(X_train_copy)
    train_pred_labels_noise = out_clf_noise.predict_label(X_train_copy,X_train_copy )
    #print("åŠ å™ªè®­ç»ƒé›†ä¸­å¼‚å¸¸å€¼åˆ¤å®šé˜ˆå€¼ä¸ºï¼š", out_clf_noise.threshold_)
    train_outliers_index_noise = []
    #print("åŠ å™ªè®­ç»ƒé›†æ ·æœ¬æ•°ï¼š", len(X_train_copy))
    for i in range(len(X_train_copy)):
        if train_pred_labels_noise[i] == 1:
            train_outliers_index_noise.append(i)
    # è®­ç»ƒæ ·æœ¬ä¸­çš„å¼‚å¸¸å€¼ç´¢å¼•
    # print("åŠ å™ªè®­ç»ƒé›†ä¸­å¼‚å¸¸å€¼ç´¢å¼•ï¼š", train_outliers_index_noise)
    # print("åŠ å™ªè®­ç»ƒé›†ä¸­çš„å¼‚å¸¸å€¼æ•°é‡ï¼š", len(train_outliers_index_noise))
    # print("åŠ å™ªè®­ç»ƒé›†ä¸­çš„å¼‚å¸¸å€¼æ¯”ä¾‹ï¼š", len(train_outliers_index_noise)/len(X_train_copy))

    # subsection ä»åŠ å™ªæµ‹è¯•é›†ä¸­æ£€æµ‹å‡ºå¼‚å¸¸å€¼ç´¢å¼•
    #test_scores_noise = out_clf_noise.decision_function(X_test_copy)
    test_pred_labels_noise = out_clf_noise.predict_label(X_train_copy, X_test_copy)
    #print("åŠ å™ªæµ‹è¯•é›†ä¸­å¼‚å¸¸å€¼åˆ¤å®šé˜ˆå€¼ä¸ºï¼š", out_clf_noise.threshold_)
    test_outliers_index_noise = []
    #print("åŠ å™ªæµ‹è¯•é›†æ ·æœ¬æ•°ï¼š", len(X_test_copy))
    for i in range(len(X_test_copy)):
        if test_pred_labels_noise[i] == 1:
            test_outliers_index_noise.append(i)
    # è®­ç»ƒæ ·æœ¬ä¸­çš„å¼‚å¸¸å€¼ç´¢å¼•
    # print("åŠ å™ªæµ‹è¯•é›†ä¸­å¼‚å¸¸å€¼ç´¢å¼•ï¼š", test_outliers_index_noise)
    # print("åŠ å™ªæµ‹è¯•é›†ä¸­çš„å¼‚å¸¸å€¼æ•°é‡ï¼š", len(test_outliers_index_noise))
    # print("åŠ å™ªæµ‹è¯•é›†ä¸­çš„å¼‚å¸¸å€¼æ¯”ä¾‹ï¼š", len(test_outliers_index_noise)/len(X_test_copy))
    # SECTION softmaxæ¨¡å‹çš„å®ç°

    # subsection åŸå§‹æ•°æ®é›†ä¸Šè®­ç»ƒçš„softmaxæ¨¡å‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­åˆ†é”™çš„æ ·æœ¬æ¯”ä¾‹

    print("*" * 100)
    softmax_model = LogisticRegression(multi_class='ovr', solver='liblinear', random_state=42, class_weight='balanced')
    softmax_model.fit(X_train, y_train)
    train_label_pred = softmax_model.predict(X_train)
    test_label_pred = softmax_model.predict(X_test)

    # è®­ç»ƒæ ·æœ¬ä¸­è¢«softmaxæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    wrong_classified_train_indices = np.where(y_train != train_label_pred)[0]
    print("è®­ç»ƒæ ·æœ¬ä¸­è¢«softmaxæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»è®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼š",
          len(wrong_classified_train_indices) / len(y_train))

    # æµ‹è¯•æ ·æœ¬ä¸­è¢«softmaxæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    wrong_classified_test_indices = np.where(y_test != test_label_pred)[0]
    print("æµ‹è¯•æ ·æœ¬ä¸­è¢«softmaxæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»æµ‹è¯•æ ·æœ¬çš„æ¯”ä¾‹ï¼š", len(wrong_classified_test_indices) / len(y_test))

    # æ•´ä½“æ•°æ®é›†Dä¸­è¢«softmaxæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    print("å®Œæ•´æ•°æ®é›†Dä¸­è¢«softmaxæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»å®Œæ•´æ•°æ®çš„æ¯”ä¾‹ï¼š",
          (len(wrong_classified_train_indices) + len(wrong_classified_test_indices)) / (len(y_train) + len(y_test)))

    # subsection åŠ å™ªæ•°æ®é›†ä¸Šè®­ç»ƒçš„softmaxæ¨¡å‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­åˆ†é”™çš„æ ·æœ¬æ¯”ä¾‹

    print("*" * 100)
    softmax_model_noise = LogisticRegression(multi_class='ovr', solver='liblinear', random_state=42,
                                             class_weight='balanced')
    softmax_model_noise.fit(X_train_copy, y_train)
    train_label_pred_noise = softmax_model_noise.predict(X_train_copy)
    test_label_pred_noise = softmax_model_noise.predict(X_test_copy)

    # åŠ å™ªè®­ç»ƒæ ·æœ¬ä¸­è¢«softmaxæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    wrong_classified_train_indices_noise = np.where(y_train != train_label_pred_noise)[0]
    print("åŠ å™ªè®­ç»ƒæ ·æœ¬ä¸­è¢«softmaxæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»åŠ å™ªè®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼š",
          len(wrong_classified_train_indices_noise) / len(y_train))

    # åŠ å™ªæµ‹è¯•æ ·æœ¬ä¸­è¢«softmaxæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    wrong_classified_test_indices_noise = np.where(y_test != test_label_pred_noise)[0]
    print("åŠ å™ªæµ‹è¯•æ ·æœ¬ä¸­è¢«softmaxæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»æµ‹è¯•æ ·æœ¬çš„æ¯”ä¾‹ï¼š",
          len(wrong_classified_test_indices_noise) / len(y_test))

    # æ•´ä½“åŠ å™ªæ•°æ®é›†Dä¸­è¢«softmaxæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    print("å®Œæ•´æ•°æ®é›†Dä¸­è¢«softmaxæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»å®Œæ•´æ•°æ®çš„æ¯”ä¾‹ï¼š",
          (len(wrong_classified_train_indices_noise) + len(wrong_classified_test_indices_noise)) / (
                      len(y_train) + len(y_test)))

    # section ä¿®å¤å‰å®éªŒæŒ‡æ ‡æµ‹å®š

    """AccuracyæŒ‡æ ‡"""
    print("*" * 100)
    print("åˆ†ç±»å™¨åœ¨ä¿®å¤å‰çš„åŠ å™ªæµ‹è¯•é›†ä¸­çš„åˆ†ç±»å‡†ç¡®åº¦ï¼š" + str(accuracy_score(y_test, test_label_pred_noise)))

    """Precision/Recall/F1æŒ‡æ ‡"""

    # average='micro': å…¨å±€è®¡ç®— F1 åˆ†æ•°ï¼Œé€‚ç”¨äºå¤„ç†ç±»åˆ«ä¸å¹³è¡¡çš„æƒ…å†µã€‚
    # average='macro': ç±»åˆ« F1 åˆ†æ•°çš„ç®€å•å¹³å‡ï¼Œé€‚ç”¨äºéœ€è¦å‡è¡¡è€ƒè™‘æ¯ä¸ªç±»åˆ«çš„æƒ…å†µã€‚
    # average='weighted': åŠ æƒ F1 åˆ†æ•°ï¼Œé€‚ç”¨äºç±»åˆ«ä¸å¹³è¡¡çš„æƒ…å†µï¼Œè€ƒè™‘äº†æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬é‡ã€‚
    # average=None: è¿”å›æ¯ä¸ªç±»åˆ«çš„ F1 åˆ†æ•°ï¼Œé€‚ç”¨äºè¯¦ç»†åˆ†ææ¯ä¸ªç±»åˆ«çš„è¡¨ç°ã€‚

    print("åˆ†ç±»å™¨åœ¨ä¿®å¤å‰çš„åŠ å™ªæµ‹è¯•é›†ä¸­çš„åˆ†ç±»ç²¾ç¡®åº¦ï¼š" + str(
        precision_score(y_test, test_label_pred_noise, average='weighted')))
    print("åˆ†ç±»å™¨åœ¨ä¿®å¤å‰çš„åŠ å™ªæµ‹è¯•é›†ä¸­çš„åˆ†ç±»å¬å›ç‡ï¼š" + str(
        recall_score(y_test, test_label_pred_noise, average='weighted')))
    print(
        "åˆ†ç±»å™¨åœ¨ä¿®å¤å‰çš„åŠ å™ªæµ‹è¯•é›†ä¸­çš„åˆ†ç±»F1åˆ†æ•°ï¼š" + str(f1_score(y_test, test_label_pred_noise, average='weighted')))

    # section è¯†åˆ«X_copyä¸­éœ€è¦ä¿®å¤çš„å…ƒç»„

    # å¼‚å¸¸æ£€æµ‹å™¨æ£€æµ‹å‡ºçš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸­çš„å¼‚å¸¸å€¼åœ¨åŸå«å™ªæ•°æ®D'ä¸­çš„ç´¢å¼•
    train_outliers_noise = train_indices[train_outliers_index_noise]
    test_outliers_noise = test_indices[test_outliers_index_noise]
    outliers_noise = np.union1d(train_outliers_noise, test_outliers_noise)

    # choice åˆ©ç”¨æŸå¤±å‡½æ•°
    # åœ¨åŠ å™ªæ•°æ®é›†D'ä¸Šè®­ç»ƒçš„softmaxæ¨¡å‹ï¼Œå…¶åˆ†ç±»é”™è¯¯çš„æ ·æœ¬åœ¨åŸå«å™ªæ•°æ®D'ä¸­çš„ç´¢å¼•
    train_wrong_clf_noise = train_indices[wrong_classified_train_indices_noise]
    test_wrong_clf_noise = test_indices[wrong_classified_test_indices_noise]
    wrong_clf_noise = np.union1d(train_wrong_clf_noise, test_wrong_clf_noise)

    # outlierså’Œåˆ†é”™æ ·æœ¬çš„å¹¶é›†
    train_union = np.union1d(train_outliers_noise, train_wrong_clf_noise)
    test_union = np.union1d(test_outliers_noise, test_wrong_clf_noise)

    # åŠ å™ªæ•°æ®é›†D'ä¸Šéœ€è¦ä¿®å¤çš„å€¼
    # éœ€è¦ä¿®å¤çš„ç‰¹å¾å’Œæ ‡ç­¾å€¼
    X_copy_repair_indices = outliers_noise  # ä¼ ç»Ÿå¼‚å¸¸æ£€æµ‹å™¨ä»…èƒ½åˆ©ç”¨å¼‚å¸¸æ£€æµ‹æŒ‡æ ‡
    # X_copy_repair_indices = np.union1d(outliers_noise, wrong_clf_noise)

    # choice ä¸åˆ©ç”¨æŸå¤±å‡½æ•°
    # X_copy_repair_indices = outliers_noise

    X_copy_repair = X_copy[X_copy_repair_indices]
    y_repair = y[X_copy_repair_indices]

    # ç”Ÿæˆä¿ç•™çš„è¡Œç´¢å¼•
    rows_to_keep = np.setdiff1d(np.arange(X_copy.shape[0]), X_copy_repair_indices)

    # ä½¿ç”¨ä¿ç•™çš„è¡Œç´¢å¼•é€‰æ‹©D'ä¸­çš„æ­£å¸¸æ•°æ®
    # æ— éœ€ä¿®å¤çš„ç‰¹å¾å’Œæ ‡ç­¾å€¼
    X_copy_inners = X_copy[rows_to_keep]
    y_inners = y[rows_to_keep]

    # section è¯†åˆ«æœ‰å½±å“åŠ›çš„ç‰¹å¾
    # choice LIME(Local Interpretable Model-Agnostic Explanation)(æ•ˆæœå¥½)

    # ç‰¹å¾æ•°å–4æˆ–6
    i = len(feature_names)
    np.random.seed(1)
    categorical_names = {}
    for feature in categorical_features:
        le = LabelEncoder()
        le.fit(data.iloc[:, feature])
        data.iloc[:, feature] = le.transform(data.iloc[:, feature])
        categorical_names[feature] = le.classes_
    explainer = LimeTabularExplainer(X_train, feature_names=feature_names, class_names=feature_names,
                                     categorical_features=categorical_features,
                                     categorical_names=categorical_names, kernel_width=3)
    # predict_proba æ–¹æ³•ç”¨äºåˆ†ç±»ä»»åŠ¡ï¼Œpredict æ–¹æ³•ç”¨äºå›å½’ä»»åŠ¡
    predict_fn = lambda x: softmax_model.predict_proba(x)
    exp = explainer.explain_instance(X_train[i], predict_fn, num_features=len(feature_names) // 2)
    # è·å–æœ€å…·å½±å“åŠ›çš„ç‰¹å¾åŠå…¶æƒé‡
    top_features = exp.as_list()
    top_feature_names = [re.search(r'([a-zA-Z_]\w*)', feature[0]).group(0).strip() for feature in top_features]
    top_k_indices = [feature_names.index(name) for name in top_feature_names]
    print("LIMEæ£€éªŒçš„æœ€æœ‰å½±å“åŠ›çš„å±æ€§çš„ç´¢å¼•ï¼š{}".format(top_k_indices))

    # section æ–¹æ¡ˆä¸€ï¼šå¯¹X_copyä¸­éœ€è¦ä¿®å¤çš„å…ƒç»„è¿›è¡Œæ ‡ç­¾ä¿®å¤ï¼ˆknnæ–¹æ³•ï¼‰
    #  éœ€è¦ä¿®å¤çš„å…ƒç»„é€šè¿‡å¼‚å¸¸å€¼æ£€æµ‹å™¨æ£€æµ‹åˆ°çš„å…ƒç»„å’Œsoftmaxåˆ†ç±»é”™è¯¯çš„å…ƒç»„å…±åŒç¡®å®šï¼ˆå–å¹¶é›†ï¼‰

    # subsection å°è¯•ä¿®å¤å¼‚å¸¸æ•°æ®çš„æ ‡ç­¾

    knn = KNeighborsClassifier(n_neighbors=3)
    knn.fit(X_copy_inners, y_inners)

    # é¢„æµ‹å¼‚å¸¸å€¼
    y_pred = knn.predict(X_copy_repair)

    # æ›¿æ¢å¼‚å¸¸å€¼
    y[X_copy_repair_indices] = y_pred
    y_train = y[train_indices]
    y_test = y[test_indices]

    # subsection é‡æ–°åœ¨ä¿®å¤åçš„æ•°æ®ä¸Šè®­ç»ƒsoftmaxæ¨¡å‹

    softmax_repair = LogisticRegression(multi_class='ovr', solver='liblinear', random_state=42, class_weight='balanced')
    softmax_repair.fit(X_train_copy, y_train)
    y_train_pred = softmax_repair.predict(X_train_copy)
    y_test_pred = softmax_repair.predict(X_test_copy)

    print("*" * 100)
    # è®­ç»ƒæ ·æœ¬ä¸­è¢«softmaxæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    wrong_classified_train_indices = np.where(y_train != y_train_pred)[0]
    print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œè®­ç»ƒæ ·æœ¬ä¸­è¢«softmaxæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»è®­ç»ƒæ ·æœ¬çš„æ¯”ä¾‹ï¼š",
          len(wrong_classified_train_indices) / len(y_train))

    # æµ‹è¯•æ ·æœ¬ä¸­è¢«softmaxæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    wrong_classified_test_indices = np.where(y_test != y_test_pred)[0]
    print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œæµ‹è¯•æ ·æœ¬ä¸­è¢«softmaxæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»æµ‹è¯•æ ·æœ¬çš„æ¯”ä¾‹ï¼š",
          len(wrong_classified_test_indices) / len(y_test))

    # æ•´ä½“æ•°æ®é›†Dä¸­è¢«softmaxæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬
    print("åŠ å™ªæ ‡ç­¾ä¿®å¤åï¼Œå®Œæ•´æ•°æ®é›†Dä¸­è¢«softmaxæ¨¡å‹é”™è¯¯åˆ†ç±»çš„æ ·æœ¬å æ€»å®Œæ•´æ•°æ®çš„æ¯”ä¾‹ï¼š",
          (len(wrong_classified_train_indices) + len(wrong_classified_test_indices)) / (len(y_train) + len(y_test)))

    return accuracy_score(y_test, y_test_pred),f1_score(y_test, y_test_pred, average='weighted'),precision_score(y_test, y_test_pred, average='weighted'),recall_score(y_test, y_test_pred, average='weighted')


if __name__ == '__main__':
    paths = [
        "../datasets/multi_class/drybean.csv",
        "../datasets/multi_class/obesity.csv",
        "../datasets/multi_class/balita.csv",
        "../datasets/multi_class/apple.csv",
        "../datasets/multi_class/adult.csv",
        "../datasets/multi_class/body.csv",
        #"D:/CodeWork/python/outlier/Rovas/normal_experiment/datasets/multi_class/financial/financial.csv",
        "../datasets/multi_class/online.csv",
        "../datasets/multi_class/star.csv",
        "../datasets/multi_class/Student.csv",
        "../datasets/real_outlier/Cardiotocography.csv",
        "../datasets/real_outlier/annthyroid.csv",
        "../datasets/real_outlier/optdigits.csv",
        "../datasets/real_outlier//PageBlocks.csv",
        "../datasets/real_outlier/pendigits.csv",
        "../datasets/real_outlier/satellite.csv",
        "../datasets/real_outlier/shuttle.csv",
        "../datasets/real_outlier/yeast.csv"
    ]
    #folder_path = "../datasets/real_outlier/"
    # folder_path= "../datasets/multi_class/"
    res_list = [[], [], [], [], []]
    for path in paths:
        acc,f1,precision,recall=run(path)
        res_list[0].append(str("{:.3g}".format(acc)))
        res_list[1].append(str("{:.3g}".format(f1)))
        res_list[2].append(str("{:.3g}".format(precision)))
        res_list[3].append(str("{:.3g}".format(recall)))
        res_list[4].append(path)

    resrow = list(zip(res_list[0], res_list[1], res_list[2], res_list[3],res_list[4]))
    df = pd.DataFrame(resrow, columns=['Accuracy', 'F1', 'Precision', 'Recall', 'Dataset'])

    # ä¿å­˜åˆ° CSV æ–‡ä»¶
    df.to_csv('softmax_results_dagmm.csv', index=False)

    print("ç»“æœå·²ä¿å­˜åˆ° results.csv æ–‡ä»¶ä¸­")